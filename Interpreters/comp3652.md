Interpreters
============

Phases of Interpretation (and Compilation)
------------------------------------------
1. Input is tokenized by a Scanner.
    A program, called the Scanner (also called a lexer or a tokenizer)
    reads over the input and turns the
    characters in it into tokens.
    A token is one or more characters that is significant as a group.
    The process of forming tokens from an input stream of characters is
    called tokenization.
    The end result of the tokenization process is a token stream.

2. The token stream is parsed by a Parser.
    A parser takes a token stream and checks it for validity.
    It is at this stage of interpretation that syntactic errors
    (invalid combinations of tokens) are found.
    The parser simply validates the input, but is able to produce an
    Intermediate Representation (IR) of the program as a side effect.
    An IR is a data structure, usually a tree, such as an Abstract
    Syntax Tree (AST).

3. IR Walking (Interpreting)
    The interpreter takes an IR and walks it, producing a value.
    For example, the expression

      `x = 5 + 10`

    would evaluate to `15` when interpreted.
    It is important to note that the value can be anything.

3a. Translation and Code Generation
    When code is compiled, it goes through the lexing and parsing
    stages, but instead of being evaluated immediately, it is
    translated to another representation, usually Object Code,
    or another language.


Syntax vs Semantics
-------------------
Syntax refers to the structure of valid tokens in a language,
whereas sematics refers to the meaning of different tokens.
Syntactical processing is done in the Parser, and Semantic processing
is done by the interpreter or code generator, in interpretation and
compilation respectively.
